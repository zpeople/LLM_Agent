{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, \"\"))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any,Tuple\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    RecursiveJsonSplitter,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "\n",
    "EMB_NAME = \"/home/zzz/RAG_demo/model/BAAI/bge-small-zh\"  \n",
    "VECTOR_PATH = os.path.join(project_dir, \"datasets/chroma_db\")\n",
    "IS_SKIP=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4042155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def parse_conversation_data(raw_data: Dict[str, Any],index) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    解析原始对话数据，转换为结构化格式\n",
    "    \n",
    "    参数:\n",
    "        raw_data: 原始对话JSON数据\n",
    "        \n",
    "    返回:\n",
    "        结构化后的对话数据和元数据\n",
    "    \"\"\"\n",
    "    # 提取对话内容\n",
    "    conversations = raw_data.get(\"conversations\", [])\n",
    "    \n",
    "    # 分离用户查询和助手回答\n",
    "    user_query = \"\"\n",
    "    assistant_answer = \"\"\n",
    "    assistant_thought = \"\"\n",
    "    for conv in conversations:\n",
    "        if conv.get(\"from\") == \"human\":\n",
    "            user_query = conv.get(\"value\", \"\").strip()\n",
    "        elif conv.get(\"from\") == \"assistant\":\n",
    "            assistant_answer = conv.get(\"value\", \"\").strip()\n",
    "  \n",
    "    # 清理用户查询\n",
    "    user_query = re.sub(r\"[\\n\\r]+\", \" \", user_query)\n",
    "    user_query = re.sub(r\" +\", \" \", user_query)\n",
    "    user_query = re.sub(r\"，+\", \"，\", user_query)\n",
    "    user_query = re.sub(r\"？+\", \"？\", user_query)\n",
    "    \n",
    "    thought_match = re.search(r\"<think>(.*?)</think>\", assistant_answer, re.DOTALL)\n",
    "    if thought_match:\n",
    "        assistant_thought = thought_match.group(1).strip()\n",
    "        # print(assistant_thought)\n",
    "    \n",
    "        \n",
    "    # 清理回答内容（去除可能的思考标记）\n",
    "    assistant_answer = re.sub(r\"<think>(.*?)</think>\", \"\", assistant_answer, flags=re.DOTALL)\n",
    "    assistant_answer = re.sub(r\"[\\n\\r]+\", \" \", assistant_answer)\n",
    "    assistant_answer = re.sub(r\" +\", \" \", assistant_answer)\n",
    "    # 构建结构化数据\n",
    "    structured_data = {\n",
    "        \"user_query\": user_query,\n",
    "        \"think\":assistant_thought,\n",
    "        \"answer\": assistant_answer,\n",
    "        \n",
    "       \n",
    "    },\n",
    "    \n",
    "    # 构建元数据\n",
    "    meta_data = {\n",
    "        \"id\": raw_data.get(\"id\", F\"{index}\"),\n",
    "        \"conversation_count\": len(conversations)\n",
    "    }\n",
    "    \n",
    "    return structured_data, meta_data\n",
    "\n",
    "def batch_process(\n",
    "    raw_data_list: List[Dict[str, Any]]\n",
    ") -> (List[Dict[str, Any]], List[Dict[str, Any]]):\n",
    "    \"\"\"批量处理数据列表，分别返回结构化数据和元数据\"\"\"\n",
    "    structured_list = []\n",
    "    meta_list = []\n",
    "\n",
    "    for i, data in enumerate(raw_data_list):\n",
    "        structured, meta = parse_conversation_data(data,index=i)\n",
    "        structured_list.append(structured)\n",
    "        meta_list.append(meta)\n",
    "\n",
    "    return structured_list, meta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99962889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def load_json(file):\n",
    "    \"\"\"\n",
    "    加载JSON\n",
    "    \"\"\"\n",
    "    name, extension = os.path.splitext(file)\n",
    "    if extension == \".json\":\n",
    "        # 处理JSON文件\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "                # 按每条instruction 拆分\n",
    "                json_data, meta_data = batch_process(json_data)\n",
    "                datas = [\n",
    "                    Document(page_content=str(chunk), metadata=meta)\n",
    "                    for chunk, meta in zip(json_data, meta_data)\n",
    "                ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading JSON file: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Document format is not supported!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"pages: {len(datas)}\")\n",
    "\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1fc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    将数据分割成块\n",
    "    :param data:\n",
    "    :param chunk_size: chunk块大小\n",
    "    :param chunk_overlap: 重叠部分大小\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    print(f\"pages: {len(data)}\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfd5618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(embedding_name):\n",
    "    \"\"\"\n",
    "    根据embedding名称加载对应的嵌入模型\n",
    "    \"\"\"\n",
    "    # 通用模型参数配置\n",
    "    model_kwargs = {\"device\": \"cuda\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}  # 归一化嵌入向量\n",
    "\n",
    "    embedding_path = os.path.join(project_dir, \"model\", embedding_name)\n",
    "    print(embedding_path)\n",
    "\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=embedding_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6d416b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store\n",
    "def create_embeddings_chroma(\n",
    "    embedding_name, chunks, persist_dir=os.path.join(project_dir, \"db/chroma_db\")\n",
    "):\n",
    "    \"\"\"\n",
    "    创建并保存 Chroma 向量库\n",
    "    \"\"\"\n",
    "    # 获取嵌入模型\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "    if not os.path.isdir(persist_dir):\n",
    "        os.mkdir(persist_dir)\n",
    "\n",
    "    # 创建向量库时指定保存路径\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,  # 指定本地保存目录\n",
    "    )\n",
    "\n",
    "    # 打印保存信息\n",
    "    print(f\"Chroma 向量库已保存到: {os.path.abspath(persist_dir)}\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def load_embeddings_chroma(embedding_name, persist_dir):\n",
    "    \"\"\"\n",
    "    加载已保存的 Chroma 向量库\n",
    "    \"\"\"\n",
    "    # 获取与创建时相同的嵌入模型（必须一致，否则向量不兼容）\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "\n",
    "    # 加载本地向量库\n",
    "    vector_store = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "\n",
    "    print(f\"Chroma 向量库已从 {os.path.abspath(persist_dir)} 加载\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a897e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serarch(vector_path=VECTOR_PATH,embedding_name=EMB_NAME,query=\"\",top_k=3):\n",
    "    vecotr_db = load_embeddings_chroma(embedding_name, vector_path)\n",
    "    results = vecotr_db.similarity_search_with_score(\n",
    "        query,\n",
    "        k=top_k,\n",
    "    )\n",
    "    serarched_datas=[]\n",
    "    # print(results)\n",
    "    for res, score in results:\n",
    "        if not IS_SKIP:\n",
    "            print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n",
    "        serarched_datas.append(res.page_content)\n",
    "    return serarched_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d15ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_local_chroma():\n",
    "    datas = load_json(os.path.join(project_dir,\"datasets/sales_data.json\"))\n",
    "    print(datas[0])\n",
    "    chunks =chunk_data(datas,chunk_size=512,chunk_overlap=100)\n",
    "    create_embeddings_chroma(EMB_NAME, chunks, VECTOR_PATH)\n",
    "\n",
    "\n",
    "# save_local_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3f2387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zzz/RAG_demo/model/BAAI/bge-small-zh\n",
      "Chroma 向量库已从 /home/zzz/LLM_Agent/src/house_demo/datasets/chroma_db 加载\n",
      "* [SIM=0.361927] ({'user_query': '贷款能办吗？法拍房是不是要全款？', 'think': '需说明特殊贷款渠道。', 'answer': ' 合作银行提供法拍贷。首付五成，利率只比基准高0.1%。这是上周客户的贷款批复书。'},) [{'conversation_count': 2, 'id': '713'}]\n",
      "* [SIM=0.371053] ({'user_query': '公积金异地贷款怎么办？我在成都缴的。', 'think': '流程简化，跨省服务。', 'answer': ' 手机登录政务网授权就行。上周西安客户两天放款，利率比商贷低1个点。'},) [{'conversation_count': 2, 'id': '546'}]\n",
      "* [SIM=0.374520] ({'user_query': '贷款需要什么材料？', 'think': '流程简化，强调便捷。', 'answer': ' 身份证加半年流水就行，其余我们搞定。现在申请明早银行就来面签，最快三天放款。'},) [{'id': '525', 'conversation_count': 2}]\n",
      "[\"({'user_query': '贷款能办吗？法拍房是不是要全款？', 'think': '需说明特殊贷款渠道。', 'answer': ' 合作银行提供法拍贷。首付五成，利率只比基准高0.1%。这是上周客户的贷款批复书。'},)\", \"({'user_query': '公积金异地贷款怎么办？我在成都缴的。', 'think': '流程简化，跨省服务。', 'answer': ' 手机登录政务网授权就行。上周西安客户两天放款，利率比商贷低1个点。'},)\", \"({'user_query': '贷款需要什么材料？', 'think': '流程简化，强调便捷。', 'answer': ' 身份证加半年流水就行，其余我们搞定。现在申请明早银行就来面签，最快三天放款。'},)\"]\n"
     ]
    }
   ],
   "source": [
    "def test_load():\n",
    "    data = get_serarch(vector_path=VECTOR_PATH,embedding_name=EMB_NAME,query=\"贷款\",top_k=3)\n",
    "    print(data)\n",
    "\n",
    "\n",
    "test_load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e3b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
